{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all necessary modules\n",
    "\n",
    "We additionally import Iterable, which makes the parameters of a PyTorch model iterable in a for-loop. For visualization, we also import pyplot and seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PyHOtIBAU_7H",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from typing import Iterable\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot the training's performance progression\n",
    "def plot_performance(train_losses, train_accs, test_losses, test_accs):\n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2)\n",
    "    fig.set_size_inches(w=15,h=5)\n",
    "    ax1.plot(train_losses, label=\"Train Loss\")\n",
    "    ax1.plot(test_losses, label=\"Test Loss\")\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend()\n",
    "    ax2.plot(train_accs, label=\"Train Accuracy\")\n",
    "    ax2.plot(test_accs, label=\"Test Accuracy\")\n",
    "    ax2.set_xlabel(\"Epoch\")\n",
    "    ax2.set_ylabel(\"Accuracy\")\n",
    "    ax2.set_ylim(0, 1)\n",
    "    ax2.legend()\n",
    "\n",
    "    sns.despine(fig)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming Exercise: DP-SGD\n",
    "\n",
    "This programming exercise implements the differentially private version of _Stochastic Gradient Descent_ called _DP-SGD_. \n",
    "\n",
    "Complete the code by filling in the TODOs:\n",
    "- Implement the clipping function that applies the norm clipping. \n",
    "- Implement the addition of adequately scaled noise to the gradient that is used to update the network's parameters. \n",
    "\n",
    "If successful, the training takes a few minutes per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ol2g3_qKU_7K",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# define a simple model with 2 fully-connected layers\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0],-1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.log_softmax(self.fc2(x), dim = 1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xvPDoqvYU_7K",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# calculate the l2-norm of the model's gradient.\n",
    "# as the model consists of several layers, we have to aggregate the gradients of all layers and calculate\n",
    "# the overall norm.\n",
    "def calc_grad_norm(parameters: Iterable[torch.Tensor], device):\n",
    "    return torch.norm(torch.stack([torch.norm(p.grad.detach()).to(device) for p in parameters]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two cells contains the actual ToDos, marked as \"TODO\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3B8AgsshU_7L",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# clip the model's gradient to max_grad_norm\n",
    "def clip_(parameters: Iterable[torch.Tensor], max_grad_norm: float, device):\n",
    "    parameters = [p for p in parameters if p.grad is not None]\n",
    "\n",
    "    ## TODO ##\n",
    "    # implement the actual clipping.\n",
    "    # tips: - remember that you can access each layer's gradient by using \"p.grad for each p in parameters\"\n",
    "    #       - you can manipulate the gradients in this function in-place without returning anything\n",
    "    #       - clipping can be implemented as a multiplication of each gradient with a scaling factor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sy8vsNtPU_7L",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, noise_multiplier, max_grad_norm):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # we use reduction='none' so that we get the loss per sample in our batch\n",
    "    criterion =  nn.NLLLoss(reduction='none')\n",
    "\n",
    "    losses = []\n",
    "    top1_acc = []\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        min_max_grad_norm = 1e15\n",
    "\n",
    "        # prepare a dict to store single gradients by its layer's name\n",
    "        clipped_grads = {name: torch.zeros_like(param, device=device) for name, param in model.named_parameters()}\n",
    "\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(data)\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        pred = output.argmax(\n",
    "                dim=1, keepdim=True\n",
    "            )\n",
    "        correct = pred.eq(target.view_as(pred)).sum().item()\n",
    "        top1_acc.append(correct / len(data))\n",
    "\n",
    "        for i in range(loss.size(0)):\n",
    "            loss[i].backward(retain_graph=True)\n",
    "\n",
    "            clip_(model.parameters(), max_grad_norm, device)\n",
    "\n",
    "            for name, param in model.named_parameters():\n",
    "                clipped_grads[name] += param.grad.detach().clone() / loss.size(0)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        ## TODO: add noise ##\n",
    "        for key in clipped_grads.keys():\n",
    "            # 1. create appropriately scaled noise\n",
    "            # tip: the function torch.normal(...) may help you\n",
    "\n",
    "\n",
    "            # 2. add the noise to our accumulated gradients\n",
    "            # clipped_grads[key] ...\n",
    "\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            param.grad = clipped_grads[name]\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(torch.mean(loss).item())\n",
    "\n",
    "    mean_loss = np.mean(losses)\n",
    "    mean_acc = np.mean(top1_acc)\n",
    "\n",
    "    print(f'Train Epoch {epoch}: \\t Loss: {mean_loss:.6f}; Acc@1: {mean_acc:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, the template contains a test function and a main function and executes everything and plots the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bLJSql9JU_7M",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for data, targets in tqdm(test_loader):\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, targets).item()\n",
    "            pred = output.argmax(\n",
    "                dim=1, keepdim=True\n",
    "            )\n",
    "            correct += pred.eq(targets.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "\n",
    "    print(\n",
    "        \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n\".format(\n",
    "            test_loss,\n",
    "            correct,\n",
    "            len(test_loader.dataset),\n",
    "            100.0 * correct / len(test_loader.dataset),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PzBFFtQMU_7M",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# the main function\n",
    "def main():\n",
    "    # define the parameters of this training\n",
    "    lr = 5e-3\n",
    "    train_batch_size = 32\n",
    "    test_batch_size = 1000\n",
    "    epochs = 10\n",
    "    c = 1.0 # clipping bound\n",
    "    sigma = 1.1 # noise multiplier\n",
    "    \n",
    "    # if available use GPU\n",
    "    cuda_available = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda:0\" if cuda_available else \"cpu\")\n",
    "    \n",
    "    # download and prepare the training set\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(\n",
    "            \"../mnist\",\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "        batch_size=train_batch_size,\n",
    "        num_workers=1,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    \n",
    "    # download and prepare the test set\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(\n",
    "            \"../mnist\",\n",
    "            train=False,\n",
    "            transform=transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "        batch_size=test_batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=1,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    # initialize the model and copy it to device's memory\n",
    "    model = Model().to(device)\n",
    "\n",
    "    # use a standard SGD optimizer\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    # define some variables to store performance metrics\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    train_accs = []\n",
    "    test_accs = []\n",
    "\n",
    "    # for each epoch...\n",
    "    for epoch in tqdm(range(1, epochs+1), desc=\"Epoch\", unit=\"epoch\"):\n",
    "        # ...perform training step and store performance metrics\n",
    "        train_loss, train_acc = train( model, device, train_loader, optimizer, epoch, sigma, c )\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "\n",
    "        # ...perform test step and store performance metrics\n",
    "        test_loss, test_acc = test(model, device, test_loader)\n",
    "        test_losses.append(test_loss)\n",
    "        test_accs.append(test_acc)\n",
    "\n",
    "    # plot the results\n",
    "    plot_performance(train_losses, train_accs, test_losses, test_accs)\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
