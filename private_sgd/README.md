# Programming Exercise: DP-SGD

This programming exercise implements the differentially private version of \emph{Stochastic Gradient Descent} called \emph{DP-SGD}. 

Complete the code by filling in the TODOs:
- Implement the clipping function that applies the norm clipping. 
- Implement the addition of adequately scaled noise to the gradient that is used to update the network's parameters. 

If successful, the training takes about 1-2 minutes per epoch.