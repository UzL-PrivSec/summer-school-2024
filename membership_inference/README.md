## Loss-based Membership Inference Attacks

# Part I: Baseline
The average training loss of a model is a good estimate of the loss values corresponding to the members. If a model suffers from overfitting, then the loss values of the training samples (members) will be quite small and the loss values of the test samples (non-members) will be quite large. Calculate the average training loss and compare it with each loss value from the  training and test data. We call samples members if their loss value is smaller than the average training loss, and vice versa for non-members. Fill in the missing gaps to complete the attack template. Your attack should have an accuracy of about 67%.

## Part II: MALT
To get a better estimate for a loss value that achieves a good separation between members and non-members, we turn to public data. Although we do not know the private data (target\_data), we do know that it consists of specific images where similar ones (shadow\_data) are publicly available. We use the public data to train a shadow model, and then we use this shadow model to extract a loss value, which serves as a threshold to distinguish members from non-members. 

We start by calculating all loss values for members and non-members. We sort the negative losses so that previously high loss values (most likely non-members) are on the left and previously low loss values (most likely members) are on the right. On this basis, we iterate over the sorted, negative losses from the left to the right and use the ground truth to calculate an accuracy for each loss as to how well it separates members from non-members. Fill in the missing gaps to complete the attack template. Your attack should have an accuracy of about 81%.